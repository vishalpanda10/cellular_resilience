{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0433b8b-ac76-423c-bc77-bbc8207e7fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/mnt/ufs18/home-230/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/mnt/ufs18/home-230/pandavis/cellular_resilience/CellResilienceModel/codebase')\n",
    "\n",
    "\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna\n",
    "\n",
    "from src.linear_regression import ElasticLinear\n",
    "from src.train import train\n",
    "from src.mlp import MLP\n",
    "from src.evaluate import evaluate\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import lightning as L\n",
    "import pickle\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c69684-f89b-4171-8611-e4ca6e753671",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccfaf629-e2d7-411c-b28a-3694b4d09dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 156831 × 900\n",
       "    obs: 'celltype', 'x_centroid', 'y_centroid', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_10_genes', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'n_counts'\n",
       "    var: 'gene_ids', 'feature_types', 'genome', 'mt', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'n_cells', 'is_nbr', 'k'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banksy_matrix = sc.read_h5ad('../data/BreastCancer10xGenomics_Rep1/exported_data/banksy_matrix.h5ad')\n",
    "banksy_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72d74682-6ee4-4dd9-b9ec-9cb7cc559153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ABCC11', 'ACTA2', 'ACTG2', 'ADAM9', 'ADGRE5', 'ADH1B', 'ADIPOQ',\n",
      "       'AGR3', 'AHSP', 'AIF1',\n",
      "       ...\n",
      "       'TUBB2B', 'TYROBP', 'UCP1', 'USP53', 'VOPP1', 'VWF', 'WARS', 'ZEB1',\n",
      "       'ZEB2', 'ZNF562'],\n",
      "      dtype='object', length=300)\n",
      "Index(['ABCC11_nbr_0', 'ACTA2_nbr_0', 'ACTG2_nbr_0', 'ADAM9_nbr_0',\n",
      "       'ADGRE5_nbr_0', 'ADH1B_nbr_0', 'ADIPOQ_nbr_0', 'AGR3_nbr_0',\n",
      "       'AHSP_nbr_0', 'AIF1_nbr_0',\n",
      "       ...\n",
      "       'TUBB2B_nbr_0', 'TYROBP_nbr_0', 'UCP1_nbr_0', 'USP53_nbr_0',\n",
      "       'VOPP1_nbr_0', 'VWF_nbr_0', 'WARS_nbr_0', 'ZEB1_nbr_0', 'ZEB2_nbr_0',\n",
      "       'ZNF562_nbr_0'],\n",
      "      dtype='object', length=300)\n",
      "Index(['ABCC11_nbr_1', 'ACTA2_nbr_1', 'ACTG2_nbr_1', 'ADAM9_nbr_1',\n",
      "       'ADGRE5_nbr_1', 'ADH1B_nbr_1', 'ADIPOQ_nbr_1', 'AGR3_nbr_1',\n",
      "       'AHSP_nbr_1', 'AIF1_nbr_1',\n",
      "       ...\n",
      "       'TUBB2B_nbr_1', 'TYROBP_nbr_1', 'UCP1_nbr_1', 'USP53_nbr_1',\n",
      "       'VOPP1_nbr_1', 'VWF_nbr_1', 'WARS_nbr_1', 'ZEB1_nbr_1', 'ZEB2_nbr_1',\n",
      "       'ZNF562_nbr_1'],\n",
      "      dtype='object', length=300)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "print(banksy_matrix.to_df().columns[0:300])\n",
    "print(banksy_matrix.to_df().columns[300:600])\n",
    "print(banksy_matrix.to_df().columns[600:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58bdd621-0b30-4616-8ad0-09e29e9475dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "celltype\n",
       "Luminal epithelial cells      61326\n",
       "T cells                       28479\n",
       "Fibroblasts                   20672\n",
       "Macrophages                   14007\n",
       "Endothelial cells             12276\n",
       "B cells                        9709\n",
       "Myoepithelial cells            9394\n",
       "Cancer cells                    968\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banksy_matrix.obs['celltype'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fa9b250-227c-4c72-8775-56243d58e4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "View of AnnData object with n_obs × n_vars = 61326 × 900\n",
       "    obs: 'celltype', 'x_centroid', 'y_centroid', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_10_genes', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'n_counts'\n",
       "    var: 'gene_ids', 'feature_types', 'genome', 'mt', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'n_cells', 'is_nbr', 'k'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celltype_adata = banksy_matrix[banksy_matrix.obs['celltype'] == 'Luminal epithelial cells  ']\n",
    "celltype_adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86dc167e-e211-4ef4-9e9a-192a027ef0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(banksy_matrix.var) // 3\n",
    "\n",
    "X = celltype_adata.X[:, n:2*n]\n",
    "G = celltype_adata.X[:, :n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "957c8f9f-1208-4673-ae33-39ba6f282137",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, G, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da23f676-6eca-4661-b871-326ea50ccb3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10f0b0a1-936d-4732-890d-204fd3118d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nresults = train(\\n    MLP,\\n    X_train,\\n    y_train,\\n    X_test,\\n    y_test,\\n    loss_fn=torch.nn.MSELoss(),\\n    learning_rate=0.05,\\n    max_epochs=100,\\n    batch_size=None,\\n    #early_stopping=True,  \\n   # patience=10,\\n)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "results = train(\n",
    "    MLP,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    learning_rate=0.05,\n",
    "    max_epochs=100,\n",
    "    batch_size=None,\n",
    "    #early_stopping=True,  \n",
    "   # patience=10,\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75ec244-50f3-440f-b963-918ff6f9b31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate(results['model'], X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77e8576d-a2d4-40f8-a81d-5adcc9f3a427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Luminal epithelial cells  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/home/pandavis/anaconda3/envs/pytorch/lib/python ...\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO: \n",
      "  | Name    | Type       | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | loss_fn | MSELoss    | 0      | train\n",
      "1 | model   | Sequential | 66.3 K | train\n",
      "-----------------------------------------------\n",
      "66.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "66.3 K    Total params\n",
      "0.265     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name    | Type       | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | loss_fn | MSELoss    | 0      | train\n",
      "1 | model   | Sequential | 66.3 K | train\n",
      "-----------------------------------------------\n",
      "66.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "66.3 K    Total params\n",
      "0.265     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffcb1a3f27a04a31b85189803e0d779d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Metrics for Luminal epithelial cells  : {'RMSE': 0.5311313, 'R2': -0.03000536755965161}\n",
      "Processing T cells  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/home/pandavis/anaconda3/envs/pytorch/lib/python ...\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO: \n",
      "  | Name    | Type       | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | loss_fn | MSELoss    | 0      | train\n",
      "1 | model   | Sequential | 66.3 K | train\n",
      "-----------------------------------------------\n",
      "66.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "66.3 K    Total params\n",
      "0.265     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name    | Type       | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | loss_fn | MSELoss    | 0      | train\n",
      "1 | model   | Sequential | 66.3 K | train\n",
      "-----------------------------------------------\n",
      "66.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "66.3 K    Total params\n",
      "0.265     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3b5fe8cd25432fa0dc9c210f95dd7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Metrics for T cells  : {'RMSE': 0.9657476, 'R2': -0.00035696845239741336}\n",
      "Processing Fibroblasts  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/home/pandavis/anaconda3/envs/pytorch/lib/python ...\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO: \n",
      "  | Name    | Type       | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | loss_fn | MSELoss    | 0      | train\n",
      "1 | model   | Sequential | 66.3 K | train\n",
      "-----------------------------------------------\n",
      "66.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "66.3 K    Total params\n",
      "0.265     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name    | Type       | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | loss_fn | MSELoss    | 0      | train\n",
      "1 | model   | Sequential | 66.3 K | train\n",
      "-----------------------------------------------\n",
      "66.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "66.3 K    Total params\n",
      "0.265     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10579c3b3fd4378ad94cbf0ee8a5576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO: Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Metrics for Fibroblasts  : {'RMSE': 0.77474564, 'R2': 0.01694711262639752}\n",
      "Processing Macrophages  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/home/pandavis/anaconda3/envs/pytorch/lib/python ...\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO: \n",
      "  | Name    | Type       | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | loss_fn | MSELoss    | 0      | train\n",
      "1 | model   | Sequential | 66.3 K | train\n",
      "-----------------------------------------------\n",
      "66.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "66.3 K    Total params\n",
      "0.265     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name    | Type       | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | loss_fn | MSELoss    | 0      | train\n",
      "1 | model   | Sequential | 66.3 K | train\n",
      "-----------------------------------------------\n",
      "66.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "66.3 K    Total params\n",
      "0.265     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4fa80d34c3949de91a277654db3ae1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO: Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Metrics for Macrophages  : {'RMSE': 0.91936445, 'R2': 0.020162461399194344}\n",
      "Processing Endothelial cells  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/home/pandavis/anaconda3/envs/pytorch/lib/python ...\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO: \n",
      "  | Name    | Type       | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | loss_fn | MSELoss    | 0      | train\n",
      "1 | model   | Sequential | 66.3 K | train\n",
      "-----------------------------------------------\n",
      "66.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "66.3 K    Total params\n",
      "0.265     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name    | Type       | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | loss_fn | MSELoss    | 0      | train\n",
      "1 | model   | Sequential | 66.3 K | train\n",
      "-----------------------------------------------\n",
      "66.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "66.3 K    Total params\n",
      "0.265     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ec344ad0744378b0d06b2c30d3edaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO: Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Metrics for Endothelial cells  : {'RMSE': 1.0093569, 'R2': -1060225781.178182}\n",
      "Processing B cells  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/home/pandavis/anaconda3/envs/pytorch/lib/python ...\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO: \n",
      "  | Name    | Type       | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | loss_fn | MSELoss    | 0      | train\n",
      "1 | model   | Sequential | 66.3 K | train\n",
      "-----------------------------------------------\n",
      "66.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "66.3 K    Total params\n",
      "0.265     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name    | Type       | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | loss_fn | MSELoss    | 0      | train\n",
      "1 | model   | Sequential | 66.3 K | train\n",
      "-----------------------------------------------\n",
      "66.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "66.3 K    Total params\n",
      "0.265     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9da0a7b7174b2b9b9f156672ffcd33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO: Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/home/pandavis/anaconda3/envs/pytorch/lib/python ...\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Metrics for B cells  : {'RMSE': 1.085326, 'R2': 0.008468816532699262}\n",
      "Processing Myoepithelial cells  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO: \n",
      "  | Name    | Type       | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | loss_fn | MSELoss    | 0      | train\n",
      "1 | model   | Sequential | 66.3 K | train\n",
      "-----------------------------------------------\n",
      "66.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "66.3 K    Total params\n",
      "0.265     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name    | Type       | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | loss_fn | MSELoss    | 0      | train\n",
      "1 | model   | Sequential | 66.3 K | train\n",
      "-----------------------------------------------\n",
      "66.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "66.3 K    Total params\n",
      "0.265     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac4eae6ce0347c7b87066b6fe37c119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Metrics for Myoepithelial cells  : {'RMSE': 0.86678183, 'R2': 0.03189465882546596}\n"
     ]
    }
   ],
   "source": [
    "cell_types = [\n",
    "    \"Luminal epithelial cells  \",\n",
    "    \"T cells  \",\n",
    "    \"Fibroblasts  \",\n",
    "    \"Macrophages  \",\n",
    "    \"Endothelial cells  \",\n",
    "    \"B cells  \",\n",
    "    \"Myoepithelial cells  \",\n",
    "]\n",
    "\n",
    "weights = {}\n",
    "\n",
    "for cell_type in cell_types:\n",
    "    print(f\"Processing {cell_type}...\")\n",
    "\n",
    "    celltype_adata = banksy_matrix[banksy_matrix.obs['celltype'] == cell_type]\n",
    "\n",
    "    #n = len(banksy_matrix.var) // 3\n",
    "    X = celltype_adata.X[:, n:2*n]\n",
    "    G = celltype_adata.X[:, :n]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, G, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train MLP model\n",
    "    results = train(\n",
    "                    MLP,\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    X_test,\n",
    "                    y_test,\n",
    "                    loss_fn=torch.nn.MSELoss(),\n",
    "                    learning_rate=0.05,\n",
    "                    max_epochs=100,\n",
    "                    batch_size=None,\n",
    "                    #early_stopping=True,  \n",
    "                   # patience=10,\n",
    "                )\n",
    "\n",
    "    metrics = evaluate(\n",
    "        model=results['model'], \n",
    "        X_test=X_test, \n",
    "        y_test=y_test\n",
    "    )\n",
    "    \n",
    "   \n",
    "    model = results['model']\n",
    "    layer_weights = []\n",
    "    \n",
    "    for layer in model.model:\n",
    "        weight = layer.weight.detach().cpu().numpy()\n",
    "        bias = layer.bias.detach().cpu().numpy()\n",
    "        layer_weights.append({\"weight\": weight, \"bias\": bias})\n",
    "\n",
    "    \n",
    "    weights[cell_type] = layer_weights # This one stores None for now.\n",
    "    \n",
    "    #np.save(f\"results/{cell_type}_mlp_weights.npy\", results[\"weights\"])\n",
    "    print(f\"MLP Metrics for {cell_type}: {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1baeb11-56e0-441f-a079-5142a3f96c65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8a614d6-76fa-4407-b1ae-d698f062fe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = results['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b7c4389-4427-496d-a38e-b6564947f205",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.model:\n",
    "        weight = layer.weight.detach().cpu().numpy()\n",
    "        bias = layer.bias.detach().cpu().numpy()\n",
    "        layer_weights.append({\"weight\": weight, \"bias\": bias})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef29cb55-cc87-4cd4-af8f-3191bf54295c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Frobenius norm for Luminal epithelial cells  : 79.04887390136719\n",
      "Total Frobenius norm for T cells  : 91.46709251403809\n",
      "Total Frobenius norm for Fibroblasts  : 87.55792045593262\n",
      "Total Frobenius norm for Macrophages  : 98.32486152648926\n",
      "Total Frobenius norm for Endothelial cells  : 90.76177597045898\n",
      "Total Frobenius norm for B cells  : 87.98000907897949\n",
      "Total Frobenius norm for Myoepithelial cells  : 171.36374282836914\n"
     ]
    }
   ],
   "source": [
    "fro_norms = {}\n",
    "\n",
    "for cell_type in cell_types:\n",
    "    weight_celltype = weights[cell_type] \n",
    "    fro_norms[cell_type] = {\"layer_norms\": [], \"total_norm\": 0}  \n",
    "    total_fro_norm = 0  \n",
    "    for layer_index, layer in enumerate(weight_celltype):\n",
    "        W = layer[\"weight\"]  \n",
    "        fro_norm = np.linalg.norm(W, ord=\"fro\")  \n",
    "        fro_norms[cell_type][\"layer_norms\"].append(fro_norm)  \n",
    "        total_fro_norm += fro_norm  \n",
    "    \n",
    "    fro_norms[cell_type][\"total_norm\"] = total_fro_norm  # Store total norm\n",
    "\n",
    "for cell_type, data in fro_norms.items():\n",
    "    #print(f\"Frobenius norms for {cell_type}: {data['layer_norms']}\")\n",
    "    print(f\"Frobenius norm for {cell_type}: {data['total_norm']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8805e6-d30b-4e4b-a122-df5bd932af27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb449ba-db27-45be-a0f3-64d25b815333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29b1ed5-615c-4536-bc1c-73a74b0915ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bcd509-d822-4792-9c6c-4bf69575acb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607f5852-8ca6-4775-b025-ae91257ffc52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82edc022-e6eb-4fdd-a2f9-a2126904ea45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f622fbe7-f24b-4f99-bc3f-3d395c28971e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddea547a-f77b-4fb1-92ab-80dc55ba1039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cbf1d1-497e-4b30-b2e3-dcb85b29d882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7b76bd-aad3-4c39-8ce8-9a0e8869011a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faba0930-e04b-47c0-91a6-52c3aa156406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
