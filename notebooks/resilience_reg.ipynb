{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0433b8b-ac76-423c-bc77-bbc8207e7fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/mnt/ufs18/home-230/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/mnt/ufs18/home-230/pandavis/cellular_resilience/CellResilienceModel/codebase')\n",
    "\n",
    "\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna\n",
    "\n",
    "from src.linear_regression import ElasticLinear\n",
    "from src.train import train\n",
    "from src.mlp import MLP\n",
    "from src.evaluate import evaluate\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import lightning as L\n",
    "import pickle\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c69684-f89b-4171-8611-e4ca6e753671",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccfaf629-e2d7-411c-b28a-3694b4d09dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 156831 × 900\n",
       "    obs: 'celltype', 'x_centroid', 'y_centroid', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_10_genes', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'n_counts'\n",
       "    var: 'gene_ids', 'feature_types', 'genome', 'mt', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'n_cells', 'is_nbr', 'k'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banksy_matrix = sc.read_h5ad('../data/BreastCancer10xGenomics_Rep1/exported_data/banksy_matrix.h5ad')\n",
    "banksy_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72d74682-6ee4-4dd9-b9ec-9cb7cc559153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ABCC11', 'ACTA2', 'ACTG2', 'ADAM9', 'ADGRE5', 'ADH1B', 'ADIPOQ',\n",
      "       'AGR3', 'AHSP', 'AIF1',\n",
      "       ...\n",
      "       'TUBB2B', 'TYROBP', 'UCP1', 'USP53', 'VOPP1', 'VWF', 'WARS', 'ZEB1',\n",
      "       'ZEB2', 'ZNF562'],\n",
      "      dtype='object', length=300)\n",
      "Index(['ABCC11_nbr_0', 'ACTA2_nbr_0', 'ACTG2_nbr_0', 'ADAM9_nbr_0',\n",
      "       'ADGRE5_nbr_0', 'ADH1B_nbr_0', 'ADIPOQ_nbr_0', 'AGR3_nbr_0',\n",
      "       'AHSP_nbr_0', 'AIF1_nbr_0',\n",
      "       ...\n",
      "       'TUBB2B_nbr_0', 'TYROBP_nbr_0', 'UCP1_nbr_0', 'USP53_nbr_0',\n",
      "       'VOPP1_nbr_0', 'VWF_nbr_0', 'WARS_nbr_0', 'ZEB1_nbr_0', 'ZEB2_nbr_0',\n",
      "       'ZNF562_nbr_0'],\n",
      "      dtype='object', length=300)\n",
      "Index(['ABCC11_nbr_1', 'ACTA2_nbr_1', 'ACTG2_nbr_1', 'ADAM9_nbr_1',\n",
      "       'ADGRE5_nbr_1', 'ADH1B_nbr_1', 'ADIPOQ_nbr_1', 'AGR3_nbr_1',\n",
      "       'AHSP_nbr_1', 'AIF1_nbr_1',\n",
      "       ...\n",
      "       'TUBB2B_nbr_1', 'TYROBP_nbr_1', 'UCP1_nbr_1', 'USP53_nbr_1',\n",
      "       'VOPP1_nbr_1', 'VWF_nbr_1', 'WARS_nbr_1', 'ZEB1_nbr_1', 'ZEB2_nbr_1',\n",
      "       'ZNF562_nbr_1'],\n",
      "      dtype='object', length=300)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "print(banksy_matrix.to_df().columns[0:300])\n",
    "print(banksy_matrix.to_df().columns[300:600])\n",
    "print(banksy_matrix.to_df().columns[600:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58bdd621-0b30-4616-8ad0-09e29e9475dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "celltype\n",
       "Luminal epithelial cells      61326\n",
       "T cells                       28479\n",
       "Fibroblasts                   20672\n",
       "Macrophages                   14007\n",
       "Endothelial cells             12276\n",
       "B cells                        9709\n",
       "Myoepithelial cells            9394\n",
       "Cancer cells                    968\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banksy_matrix.obs['celltype'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fa9b250-227c-4c72-8775-56243d58e4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "View of AnnData object with n_obs × n_vars = 61326 × 900\n",
       "    obs: 'celltype', 'x_centroid', 'y_centroid', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_10_genes', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'n_counts'\n",
       "    var: 'gene_ids', 'feature_types', 'genome', 'mt', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'n_cells', 'is_nbr', 'k'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celltype_adata = banksy_matrix[banksy_matrix.obs['celltype'] == 'Luminal epithelial cells  ']\n",
    "celltype_adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86dc167e-e211-4ef4-9e9a-192a027ef0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(banksy_matrix.var) // 3\n",
    "\n",
    "X = celltype_adata.X[:, n:2*n]\n",
    "G = celltype_adata.X[:, :n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "957c8f9f-1208-4673-ae33-39ba6f282137",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, G, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da23f676-6eca-4661-b871-326ea50ccb3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10f0b0a1-936d-4732-890d-204fd3118d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/home/pandavis/anaconda3/envs/pytorch/lib/python ...\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO: \n",
      "  | Name         | Type    | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | loss_fn      | MSELoss | 0      | train\n",
      "1 | output_layer | Linear  | 90.3 K | train\n",
      "-------------------------------------------------\n",
      "90.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "90.3 K    Total params\n",
      "0.361     Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name         | Type    | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | loss_fn      | MSELoss | 0      | train\n",
      "1 | output_layer | Linear  | 90.3 K | train\n",
      "-------------------------------------------------\n",
      "90.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "90.3 K    Total params\n",
      "0.361     Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b379cbb22f646098b2f8067175aa8dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    }
   ],
   "source": [
    "results = train(\n",
    "    ElasticLinear,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    learning_rate=0.05,\n",
    "    max_epochs=100,\n",
    "    batch_size=None,\n",
    "    #early_stopping=True,  \n",
    "   # patience=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f75ec244-50f3-440f-b963-918ff6f9b31b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m evaluate(results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m], X_test, y_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "#evaluate(results['model'], X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77e8576d-a2d4-40f8-a81d-5adcc9f3a427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Luminal epithelial cells  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/home/pandavis/anaconda3/envs/pytorch/lib/python ...\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO: \n",
      "  | Name         | Type    | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | loss_fn      | MSELoss | 0      | train\n",
      "1 | output_layer | Linear  | 90.3 K | train\n",
      "-------------------------------------------------\n",
      "90.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "90.3 K    Total params\n",
      "0.361     Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name         | Type    | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | loss_fn      | MSELoss | 0      | train\n",
      "1 | output_layer | Linear  | 90.3 K | train\n",
      "-------------------------------------------------\n",
      "90.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "90.3 K    Total params\n",
      "0.361     Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b046dc12fe8b474eba37cd44e998b358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet Metrics for Luminal epithelial cells  : {'RMSE': 0.5443497, 'R2': 0.0033676581882638957}\n",
      "Processing T cells  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/home/pandavis/anaconda3/envs/pytorch/lib/python ...\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO: \n",
      "  | Name         | Type    | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | loss_fn      | MSELoss | 0      | train\n",
      "1 | output_layer | Linear  | 90.3 K | train\n",
      "-------------------------------------------------\n",
      "90.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "90.3 K    Total params\n",
      "0.361     Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name         | Type    | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | loss_fn      | MSELoss | 0      | train\n",
      "1 | output_layer | Linear  | 90.3 K | train\n",
      "-------------------------------------------------\n",
      "90.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "90.3 K    Total params\n",
      "0.361     Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5b2cd36c3047eeab8f4ed42a0b22bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet Metrics for T cells  : {'RMSE': 0.9624231, 'R2': 0.00711890214954658}\n",
      "Processing Fibroblasts  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/home/pandavis/anaconda3/envs/pytorch/lib/python ...\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO: \n",
      "  | Name         | Type    | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | loss_fn      | MSELoss | 0      | train\n",
      "1 | output_layer | Linear  | 90.3 K | train\n",
      "-------------------------------------------------\n",
      "90.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "90.3 K    Total params\n",
      "0.361     Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name         | Type    | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | loss_fn      | MSELoss | 0      | train\n",
      "1 | output_layer | Linear  | 90.3 K | train\n",
      "-------------------------------------------------\n",
      "90.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "90.3 K    Total params\n",
      "0.361     Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b38b67d74c74de49d187cd1b82c91fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO: Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet Metrics for Fibroblasts  : {'RMSE': 0.78869045, 'R2': 0.0049568092121262805}\n",
      "Processing Macrophages  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/home/pandavis/anaconda3/envs/pytorch/lib/python ...\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO: \n",
      "  | Name         | Type    | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | loss_fn      | MSELoss | 0      | train\n",
      "1 | output_layer | Linear  | 90.3 K | train\n",
      "-------------------------------------------------\n",
      "90.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "90.3 K    Total params\n",
      "0.361     Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name         | Type    | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | loss_fn      | MSELoss | 0      | train\n",
      "1 | output_layer | Linear  | 90.3 K | train\n",
      "-------------------------------------------------\n",
      "90.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "90.3 K    Total params\n",
      "0.361     Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c1d03108ab4917865a68605b27f2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO: Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/home/pandavis/anaconda3/envs/pytorch/lib/python ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet Metrics for Macrophages  : {'RMSE': 0.9285586, 'R2': 0.007664224276295209}\n",
      "Processing Endothelial cells  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO: \n",
      "  | Name         | Type    | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | loss_fn      | MSELoss | 0      | train\n",
      "1 | output_layer | Linear  | 90.3 K | train\n",
      "-------------------------------------------------\n",
      "90.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "90.3 K    Total params\n",
      "0.361     Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name         | Type    | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | loss_fn      | MSELoss | 0      | train\n",
      "1 | output_layer | Linear  | 90.3 K | train\n",
      "-------------------------------------------------\n",
      "90.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "90.3 K    Total params\n",
      "0.361     Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3ab108c27f4acba79feb5308e89bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO: Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/home/pandavis/anaconda3/envs/pytorch/lib/python ...\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO: \n",
      "  | Name         | Type    | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | loss_fn      | MSELoss | 0      | train\n",
      "1 | output_layer | Linear  | 90.3 K | train\n",
      "-------------------------------------------------\n",
      "90.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "90.3 K    Total params\n",
      "0.361     Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name         | Type    | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | loss_fn      | MSELoss | 0      | train\n",
      "1 | output_layer | Linear  | 90.3 K | train\n",
      "-------------------------------------------------\n",
      "90.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "90.3 K    Total params\n",
      "0.361     Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet Metrics for Endothelial cells  : {'RMSE': 1.0334702, 'R2': -304205483.5310015}\n",
      "Processing B cells  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea7a1888c1a44adbdd669738a3f6409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO: Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/home/pandavis/anaconda3/envs/pytorch/lib/python ...\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO: \n",
      "  | Name         | Type    | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | loss_fn      | MSELoss | 0      | train\n",
      "1 | output_layer | Linear  | 90.3 K | train\n",
      "-------------------------------------------------\n",
      "90.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "90.3 K    Total params\n",
      "0.361     Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name         | Type    | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | loss_fn      | MSELoss | 0      | train\n",
      "1 | output_layer | Linear  | 90.3 K | train\n",
      "-------------------------------------------------\n",
      "90.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "90.3 K    Total params\n",
      "0.361     Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet Metrics for B cells  : {'RMSE': 1.0923301, 'R2': 0.006627643754308957}\n",
      "Processing Myoepithelial cells  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "/mnt/home/pandavis/anaconda3/envs/pytorch/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11a3f8ef19642c0913b3ac6f7e21076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet Metrics for Myoepithelial cells  : {'RMSE': 0.9224537, 'R2': 0.011596127755965803}\n"
     ]
    }
   ],
   "source": [
    "cell_types = [\n",
    "    \"Luminal epithelial cells  \",\n",
    "    \"T cells  \",\n",
    "    \"Fibroblasts  \",\n",
    "    \"Macrophages  \",\n",
    "    \"Endothelial cells  \",\n",
    "    \"B cells  \",\n",
    "    \"Myoepithelial cells  \",\n",
    "]\n",
    "\n",
    "weights = {}\n",
    "\n",
    "for cell_type in cell_types:\n",
    "    print(f\"Processing {cell_type}...\")\n",
    "\n",
    "    celltype_adata = banksy_matrix[banksy_matrix.obs['celltype'] == cell_type]\n",
    "\n",
    "    #n = len(banksy_matrix.var) // 3\n",
    "    X = celltype_adata.X[:, n:2*n]\n",
    "    G = celltype_adata.X[:, :n]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, G, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train ElasticLinear model\n",
    "    results = train(\n",
    "        model_class=ElasticLinear,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        loss_fn=torch.nn.MSELoss(),\n",
    "        learning_rate=0.05,\n",
    "        max_epochs=100,\n",
    "        l1_lambda=0.0,\n",
    "        l2_lambda=0.1,\n",
    "    )\n",
    "\n",
    "    metrics = evaluate(\n",
    "        model=results['model'], \n",
    "        X_test=X_test, \n",
    "        y_test=y_test\n",
    "    )\n",
    "    \n",
    "    weights[cell_type] = results['weights']\n",
    "    \n",
    "    #np.save(f\"results/{cell_type}_elastic_net_weights.npy\", results[\"weights\"])\n",
    "    print(f\"ElasticNet Metrics for {cell_type}: {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1baeb11-56e0-441f-a079-5142a3f96c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.6700308e-01,  7.3091060e-01,  9.0775937e-01, ...,\n",
       "       -2.1476422e-04,  8.5813976e-05,  5.6495512e-04], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8a614d6-76fa-4407-b1ae-d698f062fe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fro_norms = {}\n",
    "\n",
    "for cell_type in cell_types:\n",
    "\n",
    "    weight_celltype = weights[cell_type]\n",
    "    W = weight_celltype[300:].reshape(300, 300)\n",
    "\n",
    "    fro_norm = np.linalg.norm(W, ord='fro')\n",
    "    fro_norms[cell_type] = fro_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b7c4389-4427-496d-a38e-b6564947f205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Luminal epithelial cells  ': 0.114897914,\n",
       " 'T cells  ': 0.18012649,\n",
       " 'Fibroblasts  ': 0.18268244,\n",
       " 'Macrophages  ': 0.23536664,\n",
       " 'Endothelial cells  ': 0.26338494,\n",
       " 'B cells  ': 0.28131753,\n",
       " 'Myoepithelial cells  ': 0.3918243}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fro_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef29cb55-cc87-4cd4-af8f-3191bf54295c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8805e6-d30b-4e4b-a122-df5bd932af27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb449ba-db27-45be-a0f3-64d25b815333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29b1ed5-615c-4536-bc1c-73a74b0915ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bcd509-d822-4792-9c6c-4bf69575acb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607f5852-8ca6-4775-b025-ae91257ffc52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82edc022-e6eb-4fdd-a2f9-a2126904ea45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f622fbe7-f24b-4f99-bc3f-3d395c28971e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddea547a-f77b-4fb1-92ab-80dc55ba1039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cbf1d1-497e-4b30-b2e3-dcb85b29d882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7b76bd-aad3-4c39-8ce8-9a0e8869011a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faba0930-e04b-47c0-91a6-52c3aa156406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
